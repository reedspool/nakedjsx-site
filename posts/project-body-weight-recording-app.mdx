# Project: Body Weight Recording App 

[Live now]( https://reeds-website-server.fly.dev/ ) via Fly.io

## Logbook

### Sat Dec 23 03:02:52 PM PST 2023

I started this project to branch off from my <Link slug="project-supabase-authentication" hash="supabase-auth-starting-body-weight-app">exploration of Supabase authentication</Link>. Eventually, I wanted to stretch out into a diet and whole fitness application. I thought that starting with a much simpler weight tracker made more sense than sprinting for gold out of the gate.

I wanted to use this website as my application. To begin, I could use this project page as the application. Later, I'd make a dedicated page e.g. `https://reeds.website/fitness`

I began by creating a table in my Supabase database. I made it as simple as possible. It had four columns: An auto-incrementing ID, a `created_at` timestamp, a foreign key `user_id` to connect with Supabase's `auth` system which I had just figured out, and a `kilograms` field for my observed weight. I weigh myself in pounds, but it just felt weird instilling the US system of weights and measures into a database which I wanted to be maintainable. Don't at me.

<Future>I considered adding a "time" column to my weight table, so that people could update the date. I could imagine myself needing override the "created at" property when I forgot to record my observation until the next day, for example. The `created_at` column was modifiable both at insertion and update so I could always edit that. However maybe for tracking it would be good to never edit that property and always update a separate column.</Future>

<Future>I considered adding a "skipped" boolean column to my weight table, to differentiate between an intentional skipping of data entry versus a forgotten habit. I didn't have a strong use-case in mind at the time, but my favorite habit tracking app had this feature.</Future>

<Future>I added a way in my application to detect entries which duplicated the same day, so that users (me) might be prompted and have the proper resources to clean up their own data</Future>

<Future>Then I added Row Level checks to only allow users to read their own records and write records with their own user ID. I wasn't sure it worked so I had to test it. I decided to create a second email and password user and fill in some data. Then I could insert data via the API for my second user, and run a test to see whether I could query data from my first user at all. If I couldn't figure out a way to get that other data, I'd consider my Row Level policy to be working.</Future>

To get started with the client-side I began to build a quick form to enter this data. I thought that would be the most straightforward until I realized I didn't know where the data was going. Also would I have to use Supabase's JavaScript API to supply my authentication data? Or would cookies established with my authentication work so far be sufficient? Without having strong answers to those questions, I just threw down a basic HTML form to start.

<form method="post" action="">
  <ul>
    <li>
      <label>Timestamp override <input type="datetime-local" name="created_at_override" /></label>
    </li>
    <li>
      <label>Kilograms <input name="kilograms" type="number" min="0" max="99999" step="0.01" /></label>
    </li>
    <li>
      <label>Submit <input type="submit" /></label>
    </li>
  </ul>
</form>

Then I started looking at options for receiving that form data via Supabase. The only options I saw were "Edge functions" and "database functions." I didn't want to use database functions, because I wanted to return HTML via JSX to the client. When I looked into Edge functions, I found they were implemented in Deno. I didn't want to use Deno because I was afraid (without knowing for sure) that I'd have to reimplement or do extra work for my JSX/MDX implementation to work.

I considered whether I could restrict my usage to Supabase for its authentication, and then access that authentication via a NodeJS call from another server. I began looking at Fly.io, a service I knew I could put up a basic Node server for free. I'd never used Fly.io myself, but I had heard good reviews from developer friends. 

<Future>Once I had the form working with a normal, page refreshing action, I set up `htmx` to perform an insert without a page refresh. That took some work to hook up htmx and Supabase's auth token.</Future>

### Sun Dec 24 10:07:57 AM PST 2023

To get started with Fly.io, I searched for a tutorial of deploying a Docker container NodeJS application with a PostgreSQL database. Fly.io's main documentation had separate articles for each of these! 

While I read [Fly's Docker tutorial](https://fly.io/docs/languages-and-frameworks/dockerfile/) I wondered if I couldn't use a docker-compose file. A quick search told me I couldn't. So I imagined I'd be running two different applications on Fly.io. One Docker container with a database, and one with a custom Node server. And I was going to manage that Node server from the same directory as my website, since my JSX implementation and custom components already existed here, and I'd want to reuse them for both.

I wasn't enthused about my website becoming monolithic having two separate applications. I guess my fear was that because I'd done a messy job with my JSX implementation, I might have cross-polination and different concerns for different parts of the repository, leading to more work differentiating them or accidental mixing of concepts that become harder to separate later. Maybe I could just be wary of those issues moving forward, and that would be enough to prevent some disaster? I hoped so because that was my plan.

I considered not running a Postgres VM on Fly.io, and instead using my existing Postgres VM in Supabase. [Supabase's documentation on connecting to my existing database](https://supabase.com/docs/guides/database/connecting-to-postgres) was solid, so I thought, "why not?" Also in this document I learned about [`Postgres.js`](https://supabase.com/docs/guides/database/connecting-to-postgres#connecting-with-postgresjs), a package for connecting to Postgres from Node. I figured I'd be using that soon.

Fly.io [made me chuckle](https://fly.io/docs/machines/). 

> üå∂Ô∏è **But some applications get spicy.** üå∂Ô∏è This is our spicy interface!
```

I finally signed up for Fly.io and installed the `fly` CLI. The docs made it seem so easy to poke around on a brand new VM:

``

```sh
# install the `fly` CLI
curl -L https://fly.io/install.sh | sh 
# Then later (in another shell, after adding to PATH) 
# Login
flyctl auth login;
# After that succeeds
fly machine run --shell;
```

Then magically, if I could believe the docs, you'd have a terminal in a brand new machine as if you'd just SSH'd into your oldest-pal server. I tried it out and it was so close to that magical! I ended up having to take a couple detours. First to supply my credit card information to Fly.io (even though I'd only be using free stuff, hopefully). Second, I just had to login twice before the SSH connection took. I think the first time I didn't fully authenticate, perhaps because of the credit card missing? Unsure. Either way, second time was the charm!

I could `pwd`, `ls`, and `env` to my heart's content. I was even able to `apt-get update && apt-install neofetch`!

```text
            .-/+oossssoo+/-.
        `:+ssssssssssssssssss+:`
      -+ssssssssssssssssssyyssss+-
    .ossssssssssssssssssdMMMNysssso.       root@32874247b51e38 
   /ssssssssssshdmmNNmmyNMMMMhssssss/      ------------------- 
  +ssssssssshmydMMMMMMMNddddyssssssss+     OS: Ubuntu 22.04.3 LTS x86_64 
 /sssssssshNMMMyhhyyyyhmNMMMNhssssssss/    Kernel: 5.15.98-fly 
.ssssssssdMMMNhsssssssssshNMMMdssssssss.   Uptime: 4 mins 
+sssshhhyNMMNyssssssssssssyNMMMysssssss+   Packages: 231 (dpkg) 
ossyNMMMNyMMhsssssssssssssshmmmhssssssso   Shell: bash 5.1.16 
ossyNMMMNyMMhsssssssssssssshmmmhssssssso   Resolution: 1024x768 
+sssshhhyNMMNyssssssssssssyNMMMysssssss+   Terminal: hallpass 
.ssssssssdMMMNhsssssssssshNMMMdssssssss.   CPU: AMD EPYC (1) @ 2.499GHz 
 /sssssssshNMMMyhhyyyyhdNMMMNhssssssss/    Memory: 41MiB / 217MiB 
  +sssssssssdmydMMMMMMMMddddyssssssss+
   /ssssssssssshdmNNNNmyNMMMMhssssss/                              
    .ossssssssssssssssssdMMMNysssso.                               
      -+sssssssssssssssssyyyssss+-
        `:+ssssssssssssssssss+:`
            .-/+oossssoo+/-.
```

The ease to get a shell was highly encouraging, so I pushed on with my plan. 

I set out to make a NodeJS server in a Dockerfile, deploy it on Fly.io, and then
ping it via an HTTP GET from my site's client-side code.

First step, running that server locally. I searched "docker node server 2023" to refresh my brain, since I hadn't used Docker or worked on a server in over a year. I glanced at some tutorials and  

I learned [this convenient command to build and run from a Dockerfile](https://stackoverflow.com/a/51314059) in one step:

```sh
docker run --rm -it $(docker build -q .)
```

Though that was annoying because it only worked by hiding all of `docker build`'s output. I thought I'd probably just use the old fashion way of a named image.

```
docker build -t reeds-website-server . && docker run -it reeds-website-server
```

I had a lot to re-learn about Docker and images and containers and combine all of that with what was floating around in my head about TypeScript, custom JSX, and node servers. I wanted to make some smart decisions at this stage about the composition of my Dockerfile. For example, was I going to compile Typescript outside my Dockerfile, to make it simpler? Or include as much operational information in my Dockerfile, to take advantage of Docker's skills at caching operational steps. I realized I just didn't remember enough about Docker at the moment to make a perfect call, so I should just move forward with my best guess. I tried to focus on how easy and fun it could be to make the perfect Docker setup, and that my choices today weren't going to make or break the future. The internal monologue was equal parts quelling my anxiety, and accepting that I had a lot to relearn.

<Future>I considered encapsulating my static site in a Dockerfile to parallel the docker strategy of my server. I could run my compile scripts in that Dockerfile, then copy out the static build directory, then use Netlify's "deploy" command. I might be able to do this process through GitHub actions without much effort. And that way, my flow of pushing to GitHub to deploy would be uninterrupted (although I'd have to go somewhere else to track build failures versus deployment questions).</Future>

I completed my first step: a Dockerfile image running a Node Typescript app, logging some HTML text generated from JSX and my custom JSX implementation. It wasn't simple, and unfortunately it wasn't as simple as possible. I had to copy some files into my new `server` directory, because apparently my Dockerfile directives refused to copy from a parent directory. I tried using symlinks for the files, but that didn't work. It felt wrong to commit two copies of the same two files but I wanted to move forward and fix my Docker setup later.

I quickly deployed my site to Fly.io with `fly launch`. I had to wrestle with the port as Fly's CLI kept writing the port 3000 to the `fly.toml` it created, and I had to overwrite it to 3001 before deploying. I chose 3001 because my static website server `npx serve` already defaulted to 3000. It was just a coincidence that Fly.io also defaulted to 3000.

### Mon Dec 25 09:31:40 AM PST 2023

Next I tried a request to the live server from my website.

<button onclick="fetchLiveRoot(event)">Make request to live site</button>

{`
<script type="module">
  window.fetchLiveRoot = async function (event) {
    const serverUrl = "https://reeds-website-server.fly.dev/";
    try {
      const result = await fetch(serverUrl)
      event.target.innerHTML = await result.text();
    } catch (error) {
       event.target.innerHTML = "Error, see console";
       console.error("Error from fetch", error)
    }
  }
</script>
`}

That didn't work. I was able to see in Fly.io's monitoring console that my server was throwing a CORS error. I'd set up a CORS middleware intentionally, but I guess I'd have to work a bit more at configuring it. To work on that, I ran the server locally and tried the same request there:

<button onclick="fetchLocalRoot(event)">Make request to local site</button>

{`
<script type="module">
  window.fetchLocalRoot = async function (event) {
    const serverUrl = "http://localhost:3001/";
    try {
      const result = await fetch(serverUrl)
      event.target.innerHTML = await result.text();
    } catch (error) {
       event.target.innerHTML = "Error, see console";
       console.error("Error from fetch", error)
    }
  }
</script>
`}

That failed for the same CORS reason, so I edited the server to accept my localhost address as a valid origin. I got back a successful result! So I pushed my static site and my server up to see if it would now work there. It worked! Woo!

I now had many more pieces to my puzzle. The big pieces I could see left over were connecting to my Supabase Postgres server from my Fly.io backend and also authorizing via Supabase's auth system from my website login through my Fly.io backend.

### Tue Dec 26 17:00:24 PST 2023

I took a break from my big questions about auth and backends to play with my mobile computing setup. I was curious if I could operate my fly.io remote server from Termux on my Android device. I started with the same flow I did a couple days ago on my desktop: install the CLI, authenticate, and try their command to spin up a VM and connect via shell.

I immediately ran into an issue with their install script. It attempted to access `/home` and failed, since that didn't exist on Android. So I searched "fly cli termux" before trying to diagnose the issue myself. Lucky me, it turns out some others had already made a `flyctl` package for Termux. So I installed itwith `pkg install flyctl`.

I stumbled again with the next step, authenticating. That one was easier. I had to set a password. I couldn't use the "sign in with google" button. So I used the Forgt My Password flow, and then it succeeded.

I was authenticated, so I tried the last step, "fly machine run --shell;". Unfortunately, it didn't work. Command not found, `fly`. A quick search explained that `flyctl` and `fly` where the same behind the scenes, I just only got the one with the Termux package. `flyctl machine run --shell` worked great, and again I had a shell in some remote, brand new machine! That might never get old.

### Wed Dec 27 04:38:16 PM PST 2023

I made a new README for my new server work with instructions for how to build, run, and deploy my Dockerfile driven server. Yay, monolith time!

I had authentication working on the front-end, and a server working on the back-end, but I didn't have authentication on the back-end. Somehow I'd have to connect the user identity of the human on the other side of the browser to the Postgres row-level policies on the other side of my server.

I found [some documentation about how Supabase auth portends to achieve this](https://supabase.com/docs/guides/auth/server-side-rendering). I found it funny that their documentation as about Server Side Rendering, when this was what we were doing before that term was coined.

Those docs explained that Supabase returned two important, relative pieces of the auth puzzle after authentication was successful:

> 1. Access token in the form of a JWT.
> 2. Refresh token which is a randomly generated string.

I successfully observed those values in the Network tab of DevTools, in the response from a POST to `https://<my-special-string>.supabase.co/auth/v1/token?grant_type=id_token` which occurred after I used Google's sign-in button from my <Link slug="project-supabase-authentication" hash="supabase-auth-starting-body-weight-app">exploration of Supabase authentication</Link>. 

I didn't observe them set in my cookies as the docs suggested, but I thought that might be a result of my cookie settings in Chrome, `chrome://settings/cookies`, set to "Block third-party cookies." To test, that, I added my URL `https://<my-special-string>.supabase.co` to the allow list for third-party cookies. No luck.

I was able to access the refresh token and other auth information from the DevTools console with `JSON.parse(localStorage.getItem(supabaseClient.auth.storageKey)).refresh_token`.

Back Following [docs on Server-side Rendering Auth with Supabase](https://supabase.com/docs/guides/auth/server-side-rendering).

Got this code to play with state change. I observed the cookies changing after logging in with the Google Sign In button above.

export const MySpecialScriptCodeBlockRunnable = ({ scriptProps, children }) => 
<>
{`
<pre><code>&lt;script&gt;<script class="block" type="module" {...scriptProps}>
${children}
</script>&lt;/script&gt;</code></pre>
`}
</>

<details>
<summary>Code</summary>
<MySpecialScriptCodeBlockRunnable>{`
supabaseClient.auth.onAuthStateChange((event, session) => {
  if (event === 'SIGNED_OUT' || event === 'USER_DELETED') {
    // delete cookies on sign out
    const expires = new Date(0).toUTCString()
    document.cookie = \u0060my-access-token=; path=/; domain=reeds-website-server.fly.dev; expires=\${expires}; SameSite=Lax; secure\u0060
    document.cookie = \u0060my-refresh-token=; path=/; domain=reeds-website-server.fly.dev; expires=\${expires}; SameSite=Lax; secure\u0060
  } else if (event === 'SIGNED_IN' || event === 'TOKEN_REFRESHED') {
    const maxAge = 100 * 365 * 24 * 60 * 60 // 100 years, never expires
    document.cookie = \u0060my-access-token=\${session.access_token}; path=/; domain=reeds-website-server.fly.dev; max-age=\${maxAge}; SameSite=Lax; secure\u0060
    document.cookie = \u0060my-refresh-token=\${session.refresh_token}; path=/; domain=reeds-website-server.fly.dev; max-age=\${maxAge}; SameSite=Lax; secure\u0060
  }
})
`}</MySpecialScriptCodeBlockRunnable>
</details>

Then I followed their instructions on the server-side as well:

```js
const refreshToken = req.cookies['my-refresh-token']
const accessToken = req.cookies['my-access-token']

if (refreshToken && accessToken) {
  await supabase.auth.setSession({
    refresh_token: refreshToken,
    access_token: accessToken,
    {
      auth: { persistSession: false },
    }
  })
} else {
  // make sure you handle this case!
  throw new Error('User is not authenticated.')
}

// returns user information
await supabase.auth.getUser()
```

I also had to install `supabase` on my server. I didn't see any separate server-side instructions at first glance so I followed their instructions. In the browser, above, I'd used their CDN link. I was doing the other way, via NPM, for the server. I probably would eventually install this way on my client to include via `esbuild`.

```sh
npm install @supabase/supabase-js
```

I had to install the `cookie-parser` Express middleware to see cookies. Even then, I wasn't able to see my cookies! I figured it was a cross-domain problem, since my server was live at `https://reeds-website-server.fly.dev/` whereas my titular website served from `https://reeds.website`. So I tried setting my server domain explicitly on the cookie. Unfortunately, I wasn't able to see these cookies from my server. However, I had figured out how to access them from my client JS already, so I could send them explicitly in a post body. And that's exactly what I tried next.

Repeating my button-to-make-a-request strategy from above, I supplied the `request_token` and `access_token` in the POST body of an HTTP request to my server. However, I didn't end up using this button really. Because I had to test this on my live server (auth wasn't working locally at all), I ended up just copying and pasting chunks of this code into the console, instead of pushing to git constantly and waiting for Netlify to deploy my updated static site. Working in the console turned out to be very easy since I had put my `supabaseClient` in the global scope, I didn't have to repeat that initialization. 

<button onclick="fetchWithTokens(event)">Make request to live site</button>

<details>
<summary>Code</summary>
<MySpecialScriptCodeBlockRunnable>{`
  window.fetchWithTokens = async function (event) {
    const supabaseAuthStorage = JSON.parse(localStorage.getItem(supabaseClient.auth.storageKey))
    const refreshToken = supabaseAuthStorage.refresh_token;
    const accessToken = supabaseAuthStorage.access_token;
    const serverUrl = "https://reeds-website-server.fly.dev/";
    try {
      const result = await fetch(serverUrl, {
        method: "POST",
        body: JSON.stringify({ 
          refreshToken,
          accessToken,
        })
      })
      event.target.innerHTML = await result.text();
    } catch (error) {
       event.target.innerHTML = "Error, see console";
       console.error("Error from fetch", error)
    }
  }
`}</MySpecialScriptCodeBlockRunnable>
</details>

I played with the server code until I could successfully echo back those values. Then I attempted `supabase.auth.getUser()` on the server. It worked! Next I tried connecting to Supabase Postgres while correctly authenticated, and attempt to test my policies.

I manually added a row to the table I'd already created, `fitness_record_weight`. To do that, I had to use my user ID, which I could get via `supabase.auth.getUser()`. My query on the front-end was successful:

```js
const { data, error } = await supabaseClient
  .from('fitness_record_weight')
  .select('*')
```

I got one row back! So I tried it on my server. It worked swimmingly.

Now I realized this situation wasn't what I was used to. I was used to a locked-down server. In my experience, only the server had a direct connection to the database, so there was no chance that a user could make a request I hadn't personally crafted. In this case, a user could make any request they want from the browser, as I'd just demonstrated for myself above. I wasn't sure what to make of that. I supposed that I'd have to rely on the Row level policies to ensure that a user could only damage their own personal data if they decided to do such things.

<Future>I circled back to triple check that my security and usage of Row Level Security was safe</Future>

I found the [documentation about using TypeScipt with the JavaScript client](https://supabase.com/docs/reference/javascript/typescript-support). I didn't have the `supabase` CLI installed (yet), so I [downloaded the types from the dashboard](https://supabase.com/dashboard/project/yhuswwhmfuptgznlkdvv/api?page=tables-intro). I moved the file from my downloads into my project and imported it. Worked perfectly.

### Thu Dec 28 11:08:04 AM PST 2023

I felt like I had everything I needed to start making an HTMX application, so I made a new page and started on my plan to solidify the authentication flow. I copied and collected my lessons from auth so far from this project and <Link slug="project-supabase-authentication" hash="supabase-auth-starting-body-weight-app">my previous exploration into supabase auth</Link>.

I had to install `@supabase/supabase-js` for my client-side JS. When I tried to import it, I got an error:

```
Uncaught Error: Dynamic require of "stream" is not supported
```

It took some poking around to discover this was due to `stream` being a Node library. I had an inkling that it was due to Supabase's `fetch` implementation, which they described [here](https://github.com/supabase/supabase-js?tab=readme-ov-file#custom-fetch-implementation), so I tried their instructions for using the browser's built-in fetch implementation. That didn't fix anything. I saw there were [other similar issues](https://github.com/supabase/supabase-js/issues/845) which indicated a recent change in `fetch` usage. 

I decided to just use the CDN link until these things were resolved. But unfortunately I lost all TypeScript information about the client then. I tried using `import type` from the Supabase NPM package, and manually constructing a `window.supabase` type. It worked! These were the two necessary ingredients: 

```js
import type { createClient as sbCreateClient } from "@supabase/supabase-js";

declare global {
  interface Window {
    supabase: { createClient: typeof sbCreateClient };
  }
}

window.supabase.createClient // Typed correctly!
```

Then I just installed the same Supabase script tag in my page as I'd used before:

```html
<script src="https://unpkg.com/@supabase/supabase-js@2"></script>
```

And everything worked! Well, first I had to reorder my `script` tags so that my page script came before the Google Sign In script. Google checked for the global function by the name in `data-callback` on the Google tag, and it just errored out if that function didn't exist yet. So I just had to make sure my page script loaded and provided that global function before the Google script loaded. Then everything worked!

So that was my login page working. Next I wanted to go to a page which showed all my entries in a table. To do that, I made an endpoint on my server which served that table, populated via server-side authentication. This took a lot more fiddling than I had hoped, but eventually the endpoint worked flawlessly. 

I learned a valuable lesson from this fiddling. When I was playing in the DevTools console the day before, I'd made some discoveries, but I hadn't recorded the detailed differences anywhere. This is exactly what I hoped the strength of my note-taking would be, so it was frustrating. The lesson was - if you're playing somewhere outside your notes and you're learning useful things, maybe take a note!

<Future>I figured out how to use cookies for my authentication correctly, since using only POST body was annoying. I could put it in the query parameters as well, [since HTTPS hides those as well](https://stackoverflow.com/a/61043479). But cookies definitely seemed like the industry standard.</Future>

<Future>I corrected my usage of the google sign-in nonce. When I started exploring google sign in, I used a static nonce to get through it, but that eliminated the safety guarantees a nonce provides. At this point, I thought I had two options. Maybe I could set the HTML attribute `data-nonce` via JavaScript, or I could make a backend request which returned the sign-in HTML with the Nonce.</Future>

<Future>Once I had authentication working and feeling relatively nice in my new app page, I made a bunch of fake data and a way to switch on fake-data mode so that I wouldn't have to deploy my site to test.</Future>

<Future>I figured out authentication for my local machine so that I could test with real data/API timing.</Future>

### Fri Dec 29 09:13:59 AM PST 2023

Now that I had an endpoint which returned a table with live data for my real authenticated user, I could go a couple directions. I could use HTMX or similar JS to just insert the table into the DOM. But I wanted to go a little farther than that. I wanted to start with as little JavaScript as possible. I know I already used a lot of JS to do the Google Sign In, but I didn't have to. There were other auth strategies to replace that with which used little or no JS. I thought if I could keep the mindset of serving a population of minimal JS, kind of like a squint test, I could serve a basic level of need to start, and then fancy it up with JS as I went forward.

To that end, I needed my back-end to serve an entire page. So far it was only serving the table as an HTML fragment. For my back-end to serve the same page as my static site, I'd need to shuffle things around so that my back-end and front-end could read from the same files. So I started shuffling. 

I moved my server Dockerfile up to the root of the project (and renamed it `Dockerfile.server`) so that I could copy from anywhere in the project folder hierarchy and not keep two copies of files.

My face when I find that my server was taking a half second to respond: Shocked pikachu face. I added some timing logs to my server to see which step of the process was taking the longest. I tried using `process.hrtime()` but I found it difficult to use until I found [this excellent SO answer](https://stackoverflow.com/a/58711916). I discovered that setting up the supabase authentication session took around `200ms` on every connection. And then the actual database request was taking an additional 100ms. 

I suspected the long times were due to the supabase client I'd been using for the server and that it was actually built for the browser environment. It was probably hitting supabase via REST instead of maintaining a live connection to the database. I felt like I should be connecting manually to the database, maintaining my own live connection on the server, instead of using the Supabase client library that a browser would use, where that type of long-lived connection wouldn't make much sense. But then if I managed my own connection to the database, I didn't know how to authenticate user requests coming in from the client. I knew how to get the JWT access token and refresh tokens, but I didn't know how to use that to authenticate directly via a Postgres live connection. 

I remembered I'd previously found a [different supabase client focused on SSR](https://supabase.com/docs/guides/auth/server-side/creating-a-client). I figured it was time to try that. Unfortunately, that required the base package I was already using, via `@supabase/supabase-js` and thus ran into the same issues I had a couple days ago with this error:

```text
Dynamic require of "stream" is not supported
```

That led me to discover that I'd been using `esbuild` incorrectly to compile my browser code this whole time. I wanted `format: "browser"`, but I'd copied `format: "node"` straight from the static page JSX compiler. That fixed the error! I pushed to the server, since I still didn't have Google auth working locally, to test out if I saw the authentication in the cookies as I expected.

<Future>
I got rid of my CDN import of supabase, now that I'd figured out how to compile properly for the frontend
</Future>

I did not see those cookies. And I got quite frustrated in my attempts to send those cookies to my server. Then I realized, I could just host my front-end on my fly.io machine. At least for this application, then the front-end and back-end will be on the same origin, and no more issues. 

I simply had to build the page the exact same way as I had on my front-end as on my back-end, including all of the assets. I thought maybe an easy way to test if this solved the issue first before committing to rebuilding everything would be just to copy the result of the front-end build. So I set my Dockerfile to copy some chunks of the result of my front-end build, and voila! Everything worked! My cookies were set and read on the backend.

Now that I had working, reliable, sensible authentication with cookies, I could focus on other things. I wanted to start building the HTML and CSS and add the functionality of a form.

[This pen by Carter Lovelace](https://codepen.io/carterfromsl/pen/QWYMjBW) inspired me to try a neo-brutalist style. I had a direct adaptation of the pen looking nice quickly. I realized that the gray-scale might not be the most exciting for an encouraging health habit tool.

I quickly had a table and a form working, with pretty printing of the timestamps. I could enter new rows and see them immediately in my table! Things didn't look great, especially on mobile, but it was encouraging.

<Future>I played with adding some bright colors to the theme</Future>

<Future>I added a chart to track my weight over the last 30 days</Future>

<Future>I added a chart to track my weight over a dynamic time range, where I can choose all the way back since I started entering data to now, or any interval in between</Future>

<Future>I limited the history table to the last 30 days entry, and added a method to load more on demand/scroll.</Future>

<Future>I added delete and edit buttons to the history table. The delete button had a confirmation.</Future>

<Future>Now that I had a no-JS multi-page application working, I added htmx to fetch the data on page-load if authenticated. I hoped cookies would still take care of auth with no change</Future>

<Future>I tried recording my weight for a week</Future>

### Sat Dec 30 10:58:04 AM PST 2023

I recorded my first entry today. And it was easy! Since I had a bunch of test data in the table, I decided to clear it out. I could clear it out manually, or I could implement one of my desired features to delete such data from the site. I considered between putting "row action" buttons into a column on the table, or making a page to edit all the data from an entry, including deleting it, or updating the kg, or changing the time. The latter sounded like something I'd want to have either way. So I decided to make a button on each row to take you to this magical place.

<Future>As I imagined my app getting bigger and more complex, the idea of not having many tests started to scare me a bit. I wanted to keep focusing on small parts of the system, but if I broke some other functionality I wasn't manually testing at the time, I wouldn't have any way to tell until I went to use it. I thought there was a chance it could help speed up development by lowering the friction to moving forward confidently. </Future>
